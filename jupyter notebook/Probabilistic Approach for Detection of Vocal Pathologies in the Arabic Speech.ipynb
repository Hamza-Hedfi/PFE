{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements **Probabilistic Approach for Detection of Vocal Pathologies in the Arabic Speech**, a method by Naim TERBEH for detecting vocal pathologies contained in the arabic speech\n",
    "\n",
    "For more information read his paper [Probabilistic Approach for Detection of Vocal Pathologies in the Arabic Speech](https://www.researchgate.net/publication/274832247_Probabilistic_Approach_for_Detection_of_Vocal_Pathologies_in_the_Arabic_Speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phonetic Distance and Classification\n",
    "\n",
    "This task to generate the phonetic distance requires that:\n",
    "\n",
    "1. We prepare n healthy speech corpus **($C_{i}$, 1 ≤ i ≤ n)**, and for each corpus, we determinate the correspond phonetic model **$M_{i}$, (1 ≤ i ≤ n)**.\n",
    "2. We define **S = { αij; 1 ≤ i,j ≤ n and i≠j }** a set of angles that separate **Mi** and **Mj (αij=αji and αii=0)**. \n",
    "3. We define the value **Max = maximum{S}**.\n",
    "4. We define the value **δ = standard deviation{S}**.\n",
    "5. We define the value **Avg = average{S}**.\n",
    "6. We calculate **β = Max+|Avg-δ|**.\n",
    "\n",
    "To calculate the set **S**, we follow these scalar product formulas: \n",
    "\n",
    "$$M_{i} \\cdot M_{j}=\\sum_{k=1}^{n}M_{i}[k]M_{j}[k]$$\n",
    "\n",
    "$M_{i} \\cdot M_{j}=\\left \\| M_{i} \\right \\| \\cdot \\left \\| M_{j} \\right \\|\\cdot \\cos \\left (\\alpha  \\right )$ with $\\alpha$ is the angle that separates between $M_{i}$ and $M_{j}$\n",
    "\n",
    "$$\\cos \\left (\\alpha  \\right ) = \\frac{M_{i} \\cdot M_{j}}{\\left \\| M_{i} \\right \\| \\cdot \\left \\| M_{j} \\right \\|}$$\n",
    "\n",
    "For more information read the paper [Probabilistic Approach for Detection of Vocal Pathologies in the Arabic Speech](https://www.researchgate.net/publication/274832247_Probabilistic_Approach_for_Detection_of_Vocal_Pathologies_in_the_Arabic_Speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "\n",
    "class Cleaner:\n",
    "    \"\"\"\n",
    "    This class will serve for cleanning the corpus.\n",
    "    It keeps only the arabic letters in a corpus, removing punctuations, spectial caracters \n",
    "    and normalizing the arabic letters. \n",
    "    \"\"\"\n",
    "    \n",
    "    @classmethod\n",
    "    def keep_only_arabic(cls, text):\n",
    "        \"\"\" Keep only arabic letters\n",
    "        The interval [\\u0600-\\u06FF] represents utf-8 code point representation for the arabic letters\n",
    "        For more information visit https://www.utf8-chartable.de/unicode-utf8-table.pl and chose Arabic as block\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return text\n",
    "        else:\n",
    "            text = re.findall(r'[\\u0600-\\u06FF]+', text)\n",
    "            # Remove all items in the resulting list that have len(item) <= 1\n",
    "            text = [word for word in text if len(word) > 1]\n",
    "            # Rejoin the words\n",
    "            text = ' '.join(text)\n",
    "            return text\n",
    "\n",
    "    @classmethod\n",
    "    def remove_diacritics(cls, text):\n",
    "        \"\"\"Remove diacritics\"\"\"\n",
    "        arabic_diacritics = re.compile(\"\"\"\n",
    "                                     ّ    | # Tashdid\n",
    "                                     َ    | # Fatha\n",
    "                                     ً    | # Tanwin Fath\n",
    "                                     ُ    | # Damma\n",
    "                                     ٌ    | # Tanwin Damm\n",
    "                                     ِ    | # Kasra\n",
    "                                     ٍ    | # Tanwin Kasr\n",
    "                                     ْ    | # Sukun\n",
    "                                     ـ     # Tatwil/Kashida\n",
    "                                 \"\"\", re.VERBOSE)\n",
    "        if not text:\n",
    "            return text\n",
    "        else:\n",
    "            text = re.sub(arabic_diacritics, '', text)\n",
    "            return text\n",
    "\n",
    "    @classmethod\n",
    "    def remove_punctuations(cls, text):\n",
    "        \"\"\"Remove punctuations\"\"\"\n",
    "        arabic_punctuations = '''`÷×؛<>_()*&^%][ـ،/:\"؟.,'{}~¦+|!”…“–ـ'''\n",
    "        english_punctuations = string.punctuation\n",
    "        punctuations_list = arabic_punctuations + english_punctuations\n",
    "\n",
    "        if not text:\n",
    "            return text\n",
    "        else:\n",
    "            translator = str.maketrans('', '', punctuations_list)\n",
    "            return text.translate(translator)\n",
    "\n",
    "    @classmethod\n",
    "    def normalize_arabic(cls, text):\n",
    "        \"\"\"Normalize characters\"\"\"\n",
    "        if not text:\n",
    "            return text\n",
    "        else:\n",
    "            text = re.sub(\"[إأآ]\", \"ا\", text)\n",
    "            text = re.sub(\"ى\", \"ي\", text)\n",
    "            text = re.sub(\"ؤ\", \"و\", text)\n",
    "            text = re.sub(\"ئ\", \"ي\", text)\n",
    "            text = re.sub(\"ة\", \"ت\", text)\n",
    "            text = re.sub(\"گ\", \"ك\", text)\n",
    "            return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Corpus:\n",
    "    \"\"\"This class will be the numerical representation for a corpus\"\"\"\n",
    "    \n",
    "    def __init__(self, corpus=None, path_to_corpus=None):\n",
    "        # Either we get a path to a file or the corpus it self\n",
    "        # No checking if booth are None\n",
    "        # May be added later, if further development\n",
    "        self.corpus = corpus\n",
    "        self.path_to_corpus = path_to_corpus\n",
    "\n",
    "        # If path to corpus file is set\n",
    "        # read the file and store its content to self.corpus\n",
    "        if self.path_to_corpus:\n",
    "            with open(file=self.path_to_corpus, encoding=\"utf-8\") as file:\n",
    "                self.corpus = file.read()\n",
    "\n",
    "        # A dict of bi-phonemes and corresponding frequency\n",
    "        self.bi_phonemes_frequencies = self.calc_bi_ph_freq()\n",
    "        # A numpyArr of bi-phonemes frequencies\n",
    "        self.bi_phonemes_frequencies_list = np.array(list(self.bi_phonemes_frequencies.values()))\n",
    "\n",
    "    def calc_bi_ph_freq(self):\n",
    "        # Step 1\n",
    "        # Clean the corpus\n",
    "        # Keep only arabic letters\n",
    "        self.corpus = Cleaner.keep_only_arabic(self.corpus)\n",
    "        # Remove diacritics, special characters, punctuations...\n",
    "        self.corpus = Cleaner.remove_diacritics(self.corpus)\n",
    "        self.corpus = Cleaner.remove_punctuations(self.corpus)\n",
    "        # Normalize characters\n",
    "        self.corpus = Cleaner.normalize_arabic(self.corpus)\n",
    "\n",
    "        # Generate a dict with all possible bi-phonemes arrangement from the arabic alphabet\n",
    "        # with values initialized to 0.0\n",
    "        arabic_alphabet = 'ابجدهوزحطيكلمنسعفصقرشتثخذضظغ'\n",
    "        bi_phonemes_arrangement = {''.join([letter_1, letter_2]): 0.0 for letter_1 in arabic_alphabet for letter_2 in\n",
    "                                   arabic_alphabet}\n",
    "        #print(bi_phonemes_arrangement)\n",
    "        # Split the corpus into list of words\n",
    "        corpus_word_list = self.corpus.split()\n",
    "\n",
    "        bi_phonemes_from_corpus = []\n",
    "\n",
    "        # for each word in the corpus\n",
    "        for word in corpus_word_list:\n",
    "            # exemple :\n",
    "            # word = 'حمزة'\n",
    "            # i in [1, 4]\n",
    "            for i in range(1, len(word)):\n",
    "                # in this example for each itr bi_phoneme_from_word will be\n",
    "                # 'حم'\n",
    "                # 'مز'\n",
    "                # 'زة'\n",
    "                bi_phoneme_from_word = ''.join([word[i - 1], word[i]])\n",
    "                # Inc the corresponding key\n",
    "                if bi_phoneme_from_word in bi_phonemes_arrangement:\n",
    "                    bi_phonemes_arrangement[bi_phoneme_from_word] += 1\n",
    "                bi_phonemes_from_corpus.append(bi_phoneme_from_word)\n",
    "        # A dict of bi-phonemes and corresponding frequency\n",
    "        bi_phonemes_frequencies = {bi_phoneme: (bi_phoneme_count / len(bi_phonemes_from_corpus)) for\n",
    "                                   bi_phoneme, bi_phoneme_count in\n",
    "                                   bi_phonemes_arrangement.items()}\n",
    "        return bi_phonemes_frequencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "class Model:\n",
    "    \"\"\"This class will serve for determinating and hloding our magic numbers\"\"\"\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "        # A list of bi-phonemes frequencies, we'll name it models, a model for each corpus\n",
    "        self._models = []\n",
    "        self._models.extend(args)\n",
    "\n",
    "        # Initialize S (self.separating_angles)\n",
    "        # S will contain the angles separating the models\n",
    "        self.angles_between_models = [[0 for _ in range(len(args))] for _ in range(len(args))]\n",
    "\n",
    "        # Calculate the angles that separates the models,\n",
    "        # and store each value in it's appropriate location in S\n",
    "        for index1, model1 in enumerate(self._models):\n",
    "            for index2, model2 in enumerate(self._models):\n",
    "                # Calculate the scalar product\n",
    "                # We can replace @ operator with np.dot()\n",
    "                scalar_product = model1 @ model2\n",
    "                # Calculate norm product\n",
    "                norm_product = np.linalg.norm(model1) * np.linalg.norm(model2)\n",
    "                # Calculate cos(alpha)\n",
    "                # Calculate cos(alpha)\n",
    "                # cos_alpha = (model1 @ model2) / (norm(model1) * norm(model2))\n",
    "                cos_alpha = scalar_product / norm_product\n",
    "\n",
    "                # cos_alpha = np.round(cos_alpha, 10)\n",
    "\n",
    "                # if cos_alpha > 1:\n",
    "                #     cos_alpha = 1\n",
    "                # elif cos_alpha < -1:\n",
    "                #     cos_alpha = -1\n",
    "                angle = np.arccos(np.clip(cos_alpha, -1, 1))\n",
    "\n",
    "                self.angles_between_models[index1][index2] = angle\n",
    "\n",
    "        # Final step\n",
    "        # Getting avg, max and std out of S\n",
    "        # S is a symmetric matrix, so we need only the half -1 of its values\n",
    "        # The upper or lower half doesn't matter\n",
    "        # Converting S into a set\n",
    "        x = set(np.ndarray.flatten(np.array(self.angles_between_models)))\n",
    "        x.discard(0)\n",
    "        x = np.array(list(x))\n",
    "        self._max_s = np.max(x)\n",
    "        self._avg_s = np.average(x)\n",
    "        self._std_s = np.std(x)\n",
    "        # Our beloved BETA\n",
    "        self.beta = self._max_s + np.absolute(self._avg_s - self._std_s)\n",
    "\n",
    "        # A global model for later usage when verifying a speech if healthy or not\n",
    "        self.global_reference_model = np.average(np.array(self._models), axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification procedure\n",
    "\n",
    "The proposed method to classify Arabic speech can be summarized in these following\n",
    "steps:\n",
    "\n",
    "- Generation of n phonetic models of the Arabic speech (using one corpus for each phonetic model) to calculate the maximum distance between the Arabic phonetic models (phonetic distance),\n",
    "- Generation of the phonetic reference model (the average of n previous models),\n",
    "- For each new sequence to be classified, we generate the phonetic model proper to speaker (speaker can be normal, native, with disability, …),\n",
    "- Compare these two models and classify the speech in input to healthy or pathological. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "# Get betha and the gloabl ref model\n",
    "dataset_path = r'C:\\PFE\\datasets\\v1\\*.txt'\n",
    "models = []\n",
    "for file_name in glob.glob(dataset_path):\n",
    "    models.append(Corpus(path_to_corpus=file_name).bi_phonemes_frequencies_list)\n",
    "\n",
    "m = Model(*models)\n",
    "\n",
    "beta = m.beta\n",
    "GLOBAL_REF_MODEL = m.global_reference_model\n",
    "\n",
    "with open(file='text.txt', encoding=\"utf-8\") as file:\n",
    "    txt = file.read()\n",
    "\n",
    "# Test example 1\n",
    "speaker_1 = \" السلحفاة \"\n",
    "# Test example 2\n",
    "speaker_2 = \" الثلحفاة \"\n",
    "# Test example 3\n",
    "speaker_3 = \" بسم الله الرحمن الرحيم \"\n",
    "# Test example 4\n",
    "speaker_4 = \" بسم الله الرحمان الرحيم \"\n",
    "\n",
    "txt = txt + speaker_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_1</th>\n",
       "      <th>Model_2</th>\n",
       "      <th>Model_3</th>\n",
       "      <th>Model_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Model_1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.190761</td>\n",
       "      <td>0.471203</td>\n",
       "      <td>0.224548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model_2</th>\n",
       "      <td>0.190761</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.473858</td>\n",
       "      <td>0.236543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model_3</th>\n",
       "      <td>0.471203</td>\n",
       "      <td>0.473858</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.483466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model_4</th>\n",
       "      <td>0.224548</td>\n",
       "      <td>0.236543</td>\n",
       "      <td>0.483466</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model_1   Model_2   Model_3   Model_4\n",
       "Model_1  0.000000  0.190761  0.471203  0.224548\n",
       "Model_2  0.190761  0.000000  0.473858  0.236543\n",
       "Model_3  0.471203  0.473858  0.000000  0.483466\n",
       "Model_4  0.224548  0.236543  0.483466  0.000000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "result = pd.DataFrame(np.round_(m.angles_between_models, 7),\n",
    "                      columns=['Model_1', 'Model_2', 'Model_3', 'Model_4'],\n",
    "                      index=['Model_1', 'Model_2', 'Model_3', 'Model_4'])\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "# Get the speaker betha\n",
    "speaker = Model(Corpus(corpus=txt).bi_phonemes_frequencies_list, GLOBAL_REF_MODEL)\n",
    "\n",
    "phi = speaker.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification: Pathological\n"
     ]
    }
   ],
   "source": [
    "# Step 3\n",
    "# Get the classification\n",
    "if phi >= beta:\n",
    "    print ('classification: Pathological')\n",
    "else:\n",
    "    print ('classification: Healthy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
